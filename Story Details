STRY1014430 - Remove duplicate ITAM servers by name

Note: Scripted updates for reference (step 3) and attributes (step 4) should skip workflow 
and skip setting system fields by including these 2 functions on the GlideRecord
gr.setWorkflow(false);
gr.autoSysFields(false);

Acceptance criteria:
I know this is complete when scripts are built that will automate:

1. Identification

- Table: cmdb_ci_server & all children
- Discovery Source = ITAM
- Class is Server but NOT IBM Mainframe, IBM Mainframe LPAR, IBM Mainframe Linux, Server Chassis
    (OR Class Is: AIX Server, ESX Server, HPUX Server, Linux Server, Server, Solaris Server, Windows Server)
- More than 1 record by Name

For each set by Name:

2. Primary Record
- Identify primary record as last touched [most recent Updated time stamp]

3. Reference to Task
- All other records, identify if referenced on key table TASK_CI or TASK
- IF reference exists, need to update to sys_id of primary record

4. Mark for deletion
- Mark duplicate record by setting attribute column to: DELETE
- Existing Table Cleaner job will remove duplicate record

Requirement: 

1. Find the duplicate record in the cmdb_ci_server table group by name field and recently updated record using Updated field and
discovery_source as ITAM, sys_class_name is one of AIX Server, ESX Server, HPUX Server, Linux Server, Server, Solaris Server, Windows Server
so here first record is primary record and rest of the records if found then it is duplicate for those records set 'attributes' column as DELETE


2. With the duplicate records found in step 1 get the sys_id of those records and do a query in task_ci table in the ci_item field, 
if the sys_id matches then remove the value from the ci_item field in that record in task_ci table aand set the primary record sys_id of those duplicate 
records which found in step 1.

3. Add these below 2 lines to override for updating the ci_item field in the task_ci table and while setting the attribbutes in the cmds_ci_server table for the duplicate records 
gr.setWorkflow(false);
gr.autoSysFields(false);

4. Since there is a lot of duplicate rcords in the cmdb_ci_server table instead of fix script we have to go for an approach like an schedule job
to delete and update the primary record value in the task_ci table batch wise, schedule job needs to be run every hour

Answer:

executeRule();


function executeRule() {

    var nameToPrimary = {};
    var batchSize = 100;
    var count = 0;
    var duplicateProcessed = 0;

    gs.log('ARK:::[CMDB Cleanup] Deduplicate Servers & Update Task References::: Starting duplicate cleanup job...');

    try {
        //Get the all relevant server records
        var query = 'name=10-34-122-224^discovery_source=ITAM^sys_class_nameINcmdb_ci_aix_server,cmdb_ci_esx_server,cmdb_ci_hpux_server,cmdb_ci_linux_server,cmdb_ci_server,cmdb_ci_solaris_server,cmdb_ci_win_server';
        var serverGr = new GlideRecord('cmdb_ci_server');
        serverGr.addEncodedQuery(query);
        serverGr.orderBy('name');
        serverGr.orderByDesc('sys_updated_on');
        serverGr.query();

        while (serverGr.next() && count < batchSize) {

            var name = serverGr.name.toString();
            if (!name) {
                gs.info('[CMDB Cleanup] Deduplicate Servers & Update Task Reference - Skipping record with missing name = ' + serverGr.sys_id);
                continue;
            }
            if (!nameToPrimary[name]) {
                nameToPrimary[name] = serverGr.sys_id.toString();
                gs.info('ARK:::[CMDB Cleanup Job] Primary record for "' + name + '": ' + serverGR.sys_id);
                continue;
            }

            var duplicateSysId = serverGr.sys_id.toString();
            var primarySysId = nameToPrimary[name];
            gs.info('ARK:::[CMDB Cleanup Job] Found duplicate for "' + name + '" — Duplicate: ' + duplicateSysId + ' | Primary: ' + primarySysId);

            // update task_ci references by finding the duplicates CI's and updating with primmary CI server
            var taskCiGr = new GlideRecord('task_ci');
            taskCiGr.addQuery('ci_item', duplicateSysId);
            taskCiGr.query();
            while (taskCiGr.next()) {
                var taskSysId = taskCiGr.sys_id.toString();
                taskCiGr.setWorkflow(false);
                taskCiGr.autoSysFields(false);
                taskCiGr.ci_item = primarySysId;
                taskCiGr.update();
                gs.info('ARK:::[CMDB Cleanup Job] task_ci record updated: ' + taskSysId + ' | Old CI: ' + duplicateSysId + ' => New CI: ' + primarySysId);
            }

            //Mark the duplicate for the deletion in the attributes column
            serverGr.setWorkflow(false);
            serverGr.autoSysFields(false);
            serverGr.attributes = 'DELETE';
            serverGr.update();
            gs.info('ARK:::[CMDB Cleanup Job] Marked cmdb_ci_server ' + duplicateSysId + ' with attributes = DELETE');
        }

        gs.info('ARK:::[CMDB Cleanup Job] Cleanup complete. Duplicates processed: ' + duplicatesProcessed);

    } catch (globalErr) {
        gs.error('[CMDB Cleanup] Deduplicate Servers & Update Task References. Fatal error: ' + globalErr.message);

    }
}

errors:

[CMDB Cleanup] Deduplicate Servers & Update Task References. Fatal error: "serverGR" is not defined.: no thrown error





-----------------------------------------------------------------------

https://www.servicenow.com/community/developer-forum/createorupdateci-failed-import-set-error/m-p/2798210#M1074596

--------------------------------------------------------


executeRule();


function executeRule() {

    var nameToPrimary = {};
    var batchSize = 100;
    var count = 0;
    var duplicateProcessed = 0;

    //gs.info('[CMDB Cleanup Job] Deduplicate Servers & Update Task References::: Starting duplicate cleanup job...');

    try {
        //Get the all relevant server records
        var query = 'discovery_source=ITAM^sys_class_nameINcmdb_ci_aix_server,cmdb_ci_esx_server,cmdb_ci_hpux_server,cmdb_ci_linux_server,cmdb_ci_server,cmdb_ci_solaris_server,cmdb_ci_win_server^attributes!=DELETE^ORattributesISEMPTY';
        var serverGr = new GlideRecord('cmdb_ci_server');
        serverGr.addEncodedQuery(query);
        serverGr.orderBy('name');
        serverGr.orderByDesc('sys_updated_on');
        //serverGr.setLimit(batchSize);
        serverGr.query();

        while (serverGr.next() && count < batchSize) {

            var name = serverGr.name.toString();

            //Go to the next iteration if the name is empty
            if (!name) {
                continue;
            }

            //Go to the next iterations if the primary server ia found and appened in the object
            if (!nameToPrimary[name]) {
                nameToPrimary[name] = serverGr.sys_id.toString();
                //gs.info('ARK:::[CMDB Cleanup Job] Primary record for "' + name + '": ' + serverGr.sys_id);
                continue;
            }

            var duplicateSysId = serverGr.sys_id.toString();
            var primarySysId = nameToPrimary[name];

            // update task_ci references by finding the duplicates CI's and updating with primmary CI server
            var taskCiGr = new GlideRecord('task_ci');
            taskCiGr.addQuery('ci_item', duplicateSysId);
            taskCiGr.query();
            while (taskCiGr.next()) {
                var taskSysId = taskCiGr.sys_id.toString();
                taskCiGr.setWorkflow(false);
                taskCiGr.autoSysFields(false);
                taskCiGr.ci_item = primarySysId;
                taskCiGr.update();
            }

            //Mark the duplicate for the deletion in the attributes column
            serverGr.setWorkflow(false);
            serverGr.autoSysFields(false);
            serverGr.attributes = 'DELETE';
            serverGr.update();

            //Increase the count value to update the record in batch wise
            count++;
            duplicateProcessed++;
        }
        gs.info('ARK:::[CMDB Cleanup Job] Cleanup complete. Duplicates processed: ' + duplicateProcessed);


    } catch (globalErr) {
        gs.error('[CMDB Cleanup] Deduplicate Servers & Update Task References.Fatal error: ' + globalErr.message);

    }
}
--------------------------------------------------------------------------
STRY1018363 : 

Create a new data source for Casino load balance services and transform map to load the data in to the target table cmdb_ci_endpoint_http.
Give me the perfect name for the data source and transform map, and a new import set table and how it will get created from the data source.

Requirement for creating the transform map
CSV file has 4 columns:

entrypoint_fqdn
load_balancer_hostname
vastid
last_refreshed_dt

Mapping to use for HTTP(S) Endpoint [cmdb_ci_endpoint_http].  Coalesce on URL.

Install Status - default '1' (Installed)
URL - entrypoint_fqdn
Most Recent Discovery [last_discovered] - pull Date value from 'last_refreshed_dt' - incoming format example is: 2025-04-30T14:00:06.885000+00:00


Answer:

if (source.last_refreshed_dt) {
  // Get the raw string from the CSV (example: 2025-04-30T14:00:06.885000+00:00)
  var raw = source.last_refreshed_dt.toString();

  // Step 1: Remove the microseconds (.885000) using a regular expression
  var noMicroseconds = raw.replace(/\.\d+/, '');  
  // Result: "2025-04-30T14:00:06+00:00"

  // Step 2: Remove the timezone offset (+00:00) using another regex
  var cleaned = noMicroseconds.replace(/(\+|\-)\d{2}:\d{2}$/, '');  
  // Result: "2025-04-30T14:00:06"

  // Step 3: Create a GlideDateTime object from the cleaned string
  var gdt = new GlideDateTime(cleaned);  

  // Step 4: Assign it to the target field
  target.last_discovered = gdt;
}

---------------------------------------------

How to test the database indexes in the cmdb_rel_ci table

Here’s a GlideRecord test script to evaluate all three indexes on the cmdb_rel_ci table: parent, type, child, type, and child, parent, type, port.

You can run this in Scripts - Background:

// Test Index 1: parent, type
gs.info('--- Testing index: parent, type ---');
var gr1 = new GlideRecord('cmdb_rel_ci');
gr1.addQuery('parent', 'PUT_PARENT_SYS_ID_HERE');  // replace with actual sys_id
gr1.addQuery('type', 'PUT_TYPE_SYS_ID_HERE');      // replace with actual sys_id
gr1.query();
while (gr1.next()) {
    gs.info('Match (parent, type): ' + gr1.sys_id);
}

// Test Index 2: child, type
gs.info('--- Testing index: child, type ---');
var gr2 = new GlideRecord('cmdb_rel_ci');
gr2.addQuery('child', 'PUT_CHILD_SYS_ID_HERE');    // replace with actual sys_id
gr2.addQuery('type', 'PUT_TYPE_SYS_ID_HERE');      // same type or different
gr2.query();
while (gr2.next()) {
    gs.info('Match (child, type): ' + gr2.sys_id);
}

// Test Index 3: child, parent, type, port
gs.info('--- Testing index: child, parent, type, port ---');
var gr3 = new GlideRecord('cmdb_rel_ci');
gr3.addQuery('child', 'PUT_CHILD_SYS_ID_HERE');    // replace with actual sys_id
gr3.addQuery('parent', 'PUT_PARENT_SYS_ID_HERE');  // replace with actual sys_id
gr3.addQuery('type', 'PUT_TYPE_SYS_ID_HERE');      // replace with actual sys_id
gr3.addQuery('port', 'PUT_PORT_VALUE_HERE');       // replace with actual port value
gr3.query();
while (gr3.next()) {
    gs.info('Match (child, parent, type, port): ' + gr3.sys_id);
}


✅ Optional Debugging:

Before running the script, enable Debug SQL:
	1.	Navigate to System Diagnostics > Session Debug.
	2.	Click Debug SQL ON.
	3.	Then run the script and view logs under System Logs > SQL Logs to confirm which index was used.

--------------------------------------------------------------------------------------------------

Spike story: STRY1018309

Use case:
Import Set "STRY1018309_LoadBalancer_IH-ETL" has the creation of an IntegrationHub ETL job that reads the format of data from Casino to ingest Load Balancer CIs across multiple CI Classes.

This story is to automate the Load Balancer by changing the Scheduled Import 'Casino Load Balancer' from a One-Time load of an attached CSV file, to process daily (10am) by connecting to the Casino API to retrieve the payload as a CSV formatted file.

Technical design element:

Information for Casino API - The Casino API has to be authenticated (Service Account PW can be sent via secure email by Marshall) to use a token to retrieve data. Attached is a how-to guide from Eric Gu on the Casino team.

API URL:  https://network-api.casino-services.verizon.com/
UAT:          https://network-api.casino-services-uat.ebiz.verizon.com/
DEV:         https://network-api.casino-services-dev.ebiz.verizon.com/
SERVICE ACCOUNT:  svc-snow-cmdb-casino
DOMAIN:                     us-win 

Request:
This story is to automate the Load Balancer by changing the Scheduled Import 'Casino Load Balancer' from a One-Time load of an attached CSV file, to process daily (10am) by connecting to the Casino API to retrieve the payload as a CSV formatted file.

Analysis:
The documentation received on how to access network api references curl or python call.  Majority of the time spent in this sprint was done to turn it into a REST call, check to establish the connection and pull the data.  No major blockers were encountered with any firewall or connection issues.   A REST message and a REST flow action are created to test the connection.  The REST action is able to pull back a .csv file and attach it to the data source.
More research is needed to work the REST API connection into integration hub, explore the rest integration type on data source, the use of robust transform maps, error handling and use of data stream to determine the best approach.

Go Forward Plan:
Another 8 point story is needed to research more on this spike.

-----------------------------------------------------------------------------------------------------------------------------

STRY1007797 - Populate WS1 Application Bundle Id in AYS

Analysis:
Scripted REST service:
Software_WS!_Integrations

resources:
update_ws1_softwareApps - HTTP method is POST

resource path URL: /api/verw2/software_ws1_integration/update_ws1_softwareApps

script operations:

(function process(/*RESTAPIRequest*/ request, /*RESTAPIResponse*/ response) {

    var responseObj = {};
	var messageCode = "";
	var messageText = "";
	var hasException = false;
	var exceptionIs = "";
	var status = 500;

	try{
		var mandFields = [];
		var missingFields = [];
		var missing = false;

		var requestBody = request.body;
		var requestString = requestBody.dataString;
		var parsed = JSON.parse(requestString);
		
		var name = parsed.name; 
		if(name==undefined)
		{
			missing=true; 
			missingFields.push("name");
		} 
		mandFields.push("name", name);
		
		var version = parsed.version; 
		if(version==undefined)
		{
			missing=true; 
			missingFields.push("version");
		} 
		mandFields.push("version", version);
		
		var developer = parsed.developer; 
		if(developer==undefined)
		{
			missing=true; 
			missingFields.push("developer");
		} 
		mandFields.push("developer", developer);
		
		var devicetypeID = parsed.devicetypeID;
		if(devicetypeID==undefined)
		{
			missing=true; 
			missingFields.push("devicetypeID");
		} 
		mandFields.push("devicetypeID", devicetypeID);
		
		var applicationID = parsed.applicationID;
		if(applicationID ==undefined)
		{
			missing=true; 
			missingFields.push("applicationID");
		} 
		mandFields.push("applicationID", applicationID);
		
		if(this._checkMandatoryFields(mandFields, missingFields) && missing==false )
		{
			var rec = new GlideRecord("u_ws1_apps");
			rec.addQuery('u_app_id',applicationID);
			rec.query();
			if(rec.next())
				{
					status = 400;
					messageCode = "Operation Failure";
					messageText = "WorkspaceOne application already exists.";
				}
			else
				{
					rec.initialize();
					rec.u_name= name;
					rec.u_version = version;
					rec.u_manufacturer = developer;
					rec.u_platform = devicetypeID;
					rec.u_app_id = applicationID;
					if(rec.insert())
					{
						status = 200;
						messageCode = "Operation Success";
						messageText = "Successful creation of WorkspaceOne application.";
					}
					else
					{
						status = 401;
						messageCode = "Operation Failure";
						messageText = "Unable to create WorkspaceOne application.";
					}
				}
		}
		else
			{
				status = 401;
				messageCode = "Operation Failed";
				messageText = "Missing Mandatory Fields";
			}
	}

	catch(err)
	{
		messageCode = "Operation Failed";
		messageText = "Unable to create WorkspaceOne application.";
		hasException = true;
		exceptionIs = err.toString();
	}

	response.setStatus(status);
	responseObj = {
		"MessageCode": messageCode,
		"MessageText": messageText,
		"HasException": hasException,
		"Exception": exceptionIs
	};
	response.setBody(responseObj);
})(request, response);

function _checkMandatoryFields(mandFields, missingFields){
	var isAllClear = true;
	for(var i = 1; i < mandFields.length; i++){
		if(mandFields[i] == ""){
			isAllClear = false;
			missingFields.push(mandFields[i-1]);
		}i++;
	}
	return isAllClear;
}


Request body example:
{
"name":".NET Core  Runtime & Hosting Bundle",
"version":"2.2.4",
"devicetypeID":"Windows",
"developer":"Microsoft",
"applicationID":"1"
}


table is u-ws1_apps and the field needs to be added is u_bundle_id to be added in the above script

-----------------------------------------------------------------------------------------

computer accesories: STRY1010254


if (
  ritmGr1.getRowCount() == 1 &&
  ritmGr1.getDisplayValue('active') == 'true' &&
  (
    gs.getUserID() == ritmGr1.getValue('opened_by') ||
    gs.getUserID() == ritmGr1.getValue('u_requested_for')
  )
) {
  // STEP 1: Get Catalog Item Name
  var catItem = ritmGr1.cat_item.name + "";

  // STEP 2: Check if logged-in user is in NIT_SHI_PROCUREMENT
  var loggedInUser = gs.getUserID();
  var loggedInInGroup = false;
  var grpGR = new GlideRecord('sys_user_grmember');
  grpGR.addQuery('user', loggedInUser);
  grpGR.addQuery('group.name', 'NIT_SHI_PROCUREMENT');
  grpGR.query();
  if (grpGR.hasNext()) {
    loggedInInGroup = true;
  }

  // STEP 3: Check if u_requested_for is in NIT_SHI_PROCUREMENT
  var requestedFor = ritmGr1.getValue('u_requested_for');
  var requestedInGroup = false;
  if (requestedFor) {
    var reqGrpGR = new GlideRecord('sys_user_grmember');
    reqGrpGR.addQuery('user', requestedFor);
    reqGrpGR.addQuery('group.name', 'NIT_SHI_PROCUREMENT');
    reqGrpGR.query();
    if (reqGrpGR.hasNext()) {
      requestedInGroup = true;
    }
  }

  // STEP 4: Check for variable 'pr_number' value
  var prNumber = "";
  var mtom = new GlideRecord('sc_item_option_mtom');
  mtom.addQuery('request_item', ritmGr1.getUniqueValue());
  mtom.query();
  while (mtom.next()) {
    var varRef = mtom.getElement('sc_item_option').getRefRecord();
    if (varRef.name == 'pr_number') {
      prNumber = varRef.value + "";
      break;
    }
  }


var prNumber = "";
var mtom = new GlideRecord('sc_item_option_mtom');
mtom.addQuery('request_item', ritmGr1.getUniqueValue());
mtom.query();

while (mtom.next()) {
    // Step 1: Get sc_item_option sys_id
    var optionSysId = mtom.getValue('sc_item_option');
    if (!optionSysId) continue;

    // Step 2: Get sc_item_option record
    var optionGR = new GlideRecord('sc_item_option');
    if (!optionGR.get(optionSysId)) continue;

    // Step 3: Get item_option_new sys_id
    var variableDefSysId = optionGR.getValue('item_option_new');
    if (!variableDefSysId) continue;

    // Step 4: Get item_option_new record
    var variableDefGR = new GlideRecord('item_option_new');
    if (!variableDefGR.get(variableDefSysId)) continue;

    // Step 5: Check if variable name is 'pr_number'
    if (variableDefGR.name == 'pr_number') {
        prNumber = optionGR.getValue('value') + "";
        break;
    }
}




  // FINAL CONDITION
  if (
    catItem == 'Computer Accessories' &&
    prNumber != "" &&
    (requestedInGroup || loggedInInGroup)
  ) {
    data.allowReqCancel = true;
  } else {
    data.allowReqCancel = false;
  }

  // KEEP EXISTING CANCEL LOGIC
  if (input && input.action == 'cancel') {
    ritmGr1.state = '8';
    ritmGr1.stage = 'Request Cancelled';
    ritmGr1.active = 'false';
    ritmGr1.comments = "User Comments - : " + input.comments;
    ritmGr1.approval = 'rejected';
    ritmGr1.update();
    gs.addInfoMessage('Requested Item has been cancelled.');
  }
}