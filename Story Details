STRY1014430 - Remove duplicate ITAM servers by name

Note: Scripted updates for reference (step 3) and attributes (step 4) should skip workflow 
and skip setting system fields by including these 2 functions on the GlideRecord
gr.setWorkflow(false);
gr.autoSysFields(false);

Acceptance criteria:
I know this is complete when scripts are built that will automate:

1. Identification

- Table: cmdb_ci_server & all children
- Discovery Source = ITAM
- Class is Server but NOT IBM Mainframe, IBM Mainframe LPAR, IBM Mainframe Linux, Server Chassis
    (OR Class Is: AIX Server, ESX Server, HPUX Server, Linux Server, Server, Solaris Server, Windows Server)
- More than 1 record by Name

For each set by Name:

2. Primary Record
- Identify primary record as last touched [most recent Updated time stamp]

3. Reference to Task
- All other records, identify if referenced on key table TASK_CI or TASK
- IF reference exists, need to update to sys_id of primary record

4. Mark for deletion
- Mark duplicate record by setting attribute column to: DELETE
- Existing Table Cleaner job will remove duplicate record

Requirement: 

1. Find the duplicate record in the cmdb_ci_server table group by name field and recently updated record using Updated field and
discovery_source as ITAM, sys_class_name is one of AIX Server, ESX Server, HPUX Server, Linux Server, Server, Solaris Server, Windows Server
so here first record is primary record and rest of the records if found then it is duplicate for those records set 'attributes' column as DELETE


2. With the duplicate records found in step 1 get the sys_id of those records and do a query in task_ci table in the ci_item field, 
if the sys_id matches then remove the value from the ci_item field in that record in task_ci table aand set the primary record sys_id of those duplicate 
records which found in step 1.

3. Add these below 2 lines to override for updating the ci_item field in the task_ci table and while setting the attribbutes in the cmds_ci_server table for the duplicate records 
gr.setWorkflow(false);
gr.autoSysFields(false);

4. Since there is a lot of duplicate rcords in the cmdb_ci_server table instead of fix script we have to go for an approach like an schedule job
to delete and update the primary record value in the task_ci table batch wise, schedule job needs to be run every hour

Answer:

executeRule();


function executeRule() {

    var nameToPrimary = {};
    var batchSize = 100;
    var count = 0;
    var duplicateProcessed = 0;

    gs.log('ARK:::[CMDB Cleanup] Deduplicate Servers & Update Task References::: Starting duplicate cleanup job...');

    try {
        //Get the all relevant server records
        var query = 'name=10-34-122-224^discovery_source=ITAM^sys_class_nameINcmdb_ci_aix_server,cmdb_ci_esx_server,cmdb_ci_hpux_server,cmdb_ci_linux_server,cmdb_ci_server,cmdb_ci_solaris_server,cmdb_ci_win_server';
        var serverGr = new GlideRecord('cmdb_ci_server');
        serverGr.addEncodedQuery(query);
        serverGr.orderBy('name');
        serverGr.orderByDesc('sys_updated_on');
        serverGr.query();

        while (serverGr.next() && count < batchSize) {

            var name = serverGr.name.toString();
            if (!name) {
                gs.info('[CMDB Cleanup] Deduplicate Servers & Update Task Reference - Skipping record with missing name = ' + serverGr.sys_id);
                continue;
            }
            if (!nameToPrimary[name]) {
                nameToPrimary[name] = serverGr.sys_id.toString();
                gs.info('ARK:::[CMDB Cleanup Job] Primary record for "' + name + '": ' + serverGR.sys_id);
                continue;
            }

            var duplicateSysId = serverGr.sys_id.toString();
            var primarySysId = nameToPrimary[name];
            gs.info('ARK:::[CMDB Cleanup Job] Found duplicate for "' + name + '" — Duplicate: ' + duplicateSysId + ' | Primary: ' + primarySysId);

            // update task_ci references by finding the duplicates CI's and updating with primmary CI server
            var taskCiGr = new GlideRecord('task_ci');
            taskCiGr.addQuery('ci_item', duplicateSysId);
            taskCiGr.query();
            while (taskCiGr.next()) {
                var taskSysId = taskCiGr.sys_id.toString();
                taskCiGr.setWorkflow(false);
                taskCiGr.autoSysFields(false);
                taskCiGr.ci_item = primarySysId;
                taskCiGr.update();
                gs.info('ARK:::[CMDB Cleanup Job] task_ci record updated: ' + taskSysId + ' | Old CI: ' + duplicateSysId + ' => New CI: ' + primarySysId);
            }

            //Mark the duplicate for the deletion in the attributes column
            serverGr.setWorkflow(false);
            serverGr.autoSysFields(false);
            serverGr.attributes = 'DELETE';
            serverGr.update();
            gs.info('ARK:::[CMDB Cleanup Job] Marked cmdb_ci_server ' + duplicateSysId + ' with attributes = DELETE');
        }

        gs.info('ARK:::[CMDB Cleanup Job] Cleanup complete. Duplicates processed: ' + duplicatesProcessed);

    } catch (globalErr) {
        gs.error('[CMDB Cleanup] Deduplicate Servers & Update Task References. Fatal error: ' + globalErr.message);

    }
}

errors:

[CMDB Cleanup] Deduplicate Servers & Update Task References. Fatal error: "serverGR" is not defined.: no thrown error





-----------------------------------------------------------------------

https://www.servicenow.com/community/developer-forum/createorupdateci-failed-import-set-error/m-p/2798210#M1074596

--------------------------------------------------------


executeRule();


function executeRule() {

    var nameToPrimary = {};
    var batchSize = 100;
    var count = 0;
    var duplicateProcessed = 0;

    //gs.info('[CMDB Cleanup Job] Deduplicate Servers & Update Task References::: Starting duplicate cleanup job...');

    try {
        //Get the all relevant server records
        var query = 'discovery_source=ITAM^sys_class_nameINcmdb_ci_aix_server,cmdb_ci_esx_server,cmdb_ci_hpux_server,cmdb_ci_linux_server,cmdb_ci_server,cmdb_ci_solaris_server,cmdb_ci_win_server^attributes!=DELETE^ORattributesISEMPTY';
        var serverGr = new GlideRecord('cmdb_ci_server');
        serverGr.addEncodedQuery(query);
        serverGr.orderBy('name');
        serverGr.orderByDesc('sys_updated_on');
        //serverGr.setLimit(batchSize);
        serverGr.query();

        while (serverGr.next() && count < batchSize) {

            var name = serverGr.name.toString();

            //Go to the next iteration if the name is empty
            if (!name) {
                continue;
            }

            //Go to the next iterations if the primary server ia found and appened in the object
            if (!nameToPrimary[name]) {
                nameToPrimary[name] = serverGr.sys_id.toString();
                //gs.info('ARK:::[CMDB Cleanup Job] Primary record for "' + name + '": ' + serverGr.sys_id);
                continue;
            }

            var duplicateSysId = serverGr.sys_id.toString();
            var primarySysId = nameToPrimary[name];

            // update task_ci references by finding the duplicates CI's and updating with primmary CI server
            var taskCiGr = new GlideRecord('task_ci');
            taskCiGr.addQuery('ci_item', duplicateSysId);
            taskCiGr.query();
            while (taskCiGr.next()) {
                var taskSysId = taskCiGr.sys_id.toString();
                taskCiGr.setWorkflow(false);
                taskCiGr.autoSysFields(false);
                taskCiGr.ci_item = primarySysId;
                taskCiGr.update();
            }

            //Mark the duplicate for the deletion in the attributes column
            serverGr.setWorkflow(false);
            serverGr.autoSysFields(false);
            serverGr.attributes = 'DELETE';
            serverGr.update();

            //Increase the count value to update the record in batch wise
            count++;
            duplicateProcessed++;
        }
        gs.info('ARK:::[CMDB Cleanup Job] Cleanup complete. Duplicates processed: ' + duplicateProcessed);


    } catch (globalErr) {
        gs.error('[CMDB Cleanup] Deduplicate Servers & Update Task References.Fatal error: ' + globalErr.message);

    }
}
--------------------------------------------------------------------------
STRY1018363 : 

Create a new data source for Casino load balance services and transform map to load the data in to the target table cmdb_ci_endpoint_http.
Give me the perfect name for the data source and transform map, and a new import set table and how it will get created from the data source.

Requirement for creating the transform map
CSV file has 4 columns:

entrypoint_fqdn
load_balancer_hostname
vastid
last_refreshed_dt

Mapping to use for HTTP(S) Endpoint [cmdb_ci_endpoint_http].  Coalesce on URL.

Install Status - default '1' (Installed)
URL - entrypoint_fqdn
Most Recent Discovery [last_discovered] - pull Date value from 'last_refreshed_dt' - incoming format example is: 2025-04-30T14:00:06.885000+00:00



